<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Kaggle - Whiteghost Docs</title>
        <link href="../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../..">Whiteghost Docs</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../.." class="nav-link">首页</a>
                            </li>
                            <li class="navitem">
                                <a href="../../Mkdocs/" class="nav-link">Mkdocs</a>
                            </li>
                            <li class="navitem">
                                <a href="../../Markdown%E6%95%99%E7%A8%8B/Markdown%E6%95%99%E7%A8%8B/" class="nav-link">Markdown教程</a>
                            </li>
                            <li class="navitem">
                                <a href="../../flask/flask/" class="nav-link">Flask</a>
                            </li>
                            <li class="navitem">
                                <a href="../../%E8%A5%BF%E7%8F%AD%E7%89%99%E5%9B%BD%E9%99%85%E5%BF%97%E6%84%BF%E8%80%85%E5%9B%9E%E5%BF%86%E5%BD%95/%E8%A5%BF%E7%8F%AD%E7%89%99%E5%9B%BD%E9%99%85%E5%BF%97%E6%84%BF%E8%80%85%E5%9B%9E%E5%BF%86%E5%BD%95/" class="nav-link">西班牙国际志愿者回忆录</a>
                            </li>
                            <li class="navitem active">
                                <a href="./" class="nav-link">Kaggle</a>
                            </li>
                            <li class="navitem">
                                <a href="../../%E6%9F%90%E4%BA%BA%E9%A3%9F%E8%B0%B1/" class="nav-link">某人食谱</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../../%E8%A5%BF%E7%8F%AD%E7%89%99%E5%9B%BD%E9%99%85%E5%BF%97%E6%84%BF%E8%80%85%E5%9B%9E%E5%BF%86%E5%BD%95/%E8%A5%BF%E7%8F%AD%E7%89%99%E5%9B%BD%E9%99%85%E5%BF%97%E6%84%BF%E8%80%85%E5%9B%9E%E5%BF%86%E5%BD%95/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../%E6%9F%90%E4%BA%BA%E9%A3%9F%E8%B0%B1/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#kaggle" class="nav-link">Kaggle</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#_1" class="nav-link">缘起</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#kaggle_1" class="nav-link">Kaggle的学习</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#how-models-work" class="nav-link">机器学习模型是怎么工作的（How Models Work）</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#_2" class="nav-link">特征工程</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="kaggle">Kaggle</h1>
<h2 id="_1">缘起</h2>
<p><a href="https://www.kaggle.com/">Kaggle</a>是一个数据挖掘的学习交流的论坛，从我的“收藏夹”来看，我好像很早就见过它，根据它前后的其他页面来看，大约是2020年左右吧。它再次引起的我的注意的时候，是2023年12月份我在中油绿电新能源有限公司实习的时候，看到的另一位同学投递过来的实习生应聘简历，和我一样没有导师项目的他在简历上写了“具有kaggle、天池相关项目经历”。至此我才仔细开始了解这个有意思的平台。</p>
<p>Kaggle成立于2010年，是一个进行数据发掘和预测竞赛的在线平台。从公司的角度来讲，可以提供一些数据，进而提出一个实际需要解决的问题；从参赛者的角度来讲，他们将组队参与项目，针对其中一个问题提出解决方案，最终由公司选出的最佳方案可以获得5K-10K美金的奖金。</p>
<p>除此之外，Kaggle官方每年还会举办一次大规模的竞赛，奖金高达一百万美金，吸引了广大的数据科学爱好者参与其中。从某种角度来讲，大家可以把它理解为一个众包平台，类似国内的猪八戒。但是不同于传统的低层次劳动力需求，Kaggle一直致力于解决业界难题，因此也创造了一种全新的劳动力市场——不再以学历和工作经验作为唯一的人才评判标准，而是着眼于个人技能，为顶尖人才和公司之间搭建了一座桥梁。</p>
<p>本文我致力于记录一些通过对于kaggle课程的翻译而及描述进行的数据科学相关知识的学习，之后可能会再裂变出另一个页面用来记载我在上面的各个比赛进行的尝试。</p>
<h1 id="kaggle_1">Kaggle的学习</h1>
<h2 id="how-models-work">机器学习模型是怎么工作的（How Models Work）</h2>
<p>我选择这一课作为整篇学习记录起始，因为我坚信只有更直观的知道一个东西能做什么，能有什么用，人才会有学习的动力。</p>
<p>在<a href="https://www.kaggle.com/code/dansbecker/how-models-work/tutorial"><strong>How Models Work</strong></a>这一章中，kaggle通过一个实例使用决策树模型尝试解释机器学习模型是怎么工作的。解释的场景是这样的：</p>
<ul>
<li>
<p>你的表弟通过投机房地产赚了几百万美元。因为你对数据科学感兴趣，他提出要和你成为生意伙伴。他提供资金，你提供预测各种房屋价值的模型。</p>
</li>
<li>
<p>你问表弟过去是如何预测房地产价值的，他说这只是直觉。但多问几句就会发现，他从过去看过的房子中找出了价格模型，并利用这些模型对他正在考虑的新房子进行预测。</p>
</li>
</ul>
<h3 id="1">1.分析数据</h3>
<p>任何机器学习项目的第一步都是熟悉数据，课程中，使用pandas读取了原始数据之后使用describe()方法进行了数据的数据的描述，该方法可以以一个表格的方式较为概括性的描述一下数据。</p>
<p>代码：</p>
<pre><code class="language-python"># save filepath to variable for easier access
melbourne_file_path - '../input/melbourne-housing-snapshot/melb_data.csv'
# read the data and store data in DataFrame titled melbourne_data
melbourne_data - pd.read_csv(melbourne_file_path) 
# print a summary of the data in Melbourne data
melbourne_data.describe()
</code></pre>
<p>输出：</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Rooms</th>
<th align="center">Price</th>
<th align="center">Distance</th>
<th align="center">Postcode</th>
<th align="center">Bedroom2</th>
<th align="center">Bathroom</th>
<th align="center">Car</th>
<th align="center">Landsize</th>
<th align="center">BuildingArea</th>
<th align="center">YearBuilt</th>
<th align="center">Lattitude</th>
<th align="center">Longtitude</th>
<th align="center">Propertycount</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">count</td>
<td align="center">13580.000000</td>
<td align="center">1.358000e+04</td>
<td align="center">13580.000000</td>
<td align="center">13580.000000</td>
<td align="center">13580.000000</td>
<td align="center">13580.000000</td>
<td align="center">13518.000000</td>
<td align="center">13580.000000</td>
<td align="center">7130.000000</td>
<td align="center">8205.000000</td>
<td align="center">13580.000000</td>
<td align="center">13580.000000</td>
<td align="center">13580.000000</td>
</tr>
<tr>
<td align="center">mean</td>
<td align="center">2.937997</td>
<td align="center">1.075684e+06</td>
<td align="center">10.137776</td>
<td align="center">3105.301915</td>
<td align="center">2.914728</td>
<td align="center">1.534242</td>
<td align="center">1.610075</td>
<td align="center">558.416127</td>
<td align="center">151.967650</td>
<td align="center">1964.684217</td>
<td align="center">-37.809203</td>
<td align="center">144.995216</td>
<td align="center">7454.417378</td>
</tr>
<tr>
<td align="center">std</td>
<td align="center">0.955748</td>
<td align="center">6.393107e+05</td>
<td align="center">5.868725</td>
<td align="center">90.676964</td>
<td align="center">0.965921</td>
<td align="center">0.691712</td>
<td align="center">0.962634</td>
<td align="center">3990.669241</td>
<td align="center">541.014538</td>
<td align="center">37.273762</td>
<td align="center">0.079260</td>
<td align="center">0.103916</td>
<td align="center">4378.581772</td>
</tr>
<tr>
<td align="center">min</td>
<td align="center">1.000000</td>
<td align="center">8.500000e+04</td>
<td align="center">0.000000</td>
<td align="center">3000.000000</td>
<td align="center">0.000000</td>
<td align="center">0.000000</td>
<td align="center">0.000000</td>
<td align="center">0.000000</td>
<td align="center">0.000000</td>
<td align="center">1196.000000</td>
<td align="center">-38.182550</td>
<td align="center">144.431810</td>
<td align="center">249.000000</td>
</tr>
<tr>
<td align="center">25%</td>
<td align="center">2.000000</td>
<td align="center">6.500000e+05</td>
<td align="center">6.100000</td>
<td align="center">3044.000000</td>
<td align="center">2.000000</td>
<td align="center">1.000000</td>
<td align="center">1.000000</td>
<td align="center">177.000000</td>
<td align="center">93.000000</td>
<td align="center">1940.000000</td>
<td align="center">-37.856822</td>
<td align="center">144.929600</td>
<td align="center">4380.000000</td>
</tr>
<tr>
<td align="center">50%</td>
<td align="center">3.000000</td>
<td align="center">9.030000e+05</td>
<td align="center">9.200000</td>
<td align="center">3084.000000</td>
<td align="center">3.000000</td>
<td align="center">1.000000</td>
<td align="center">2.000000</td>
<td align="center">440.000000</td>
<td align="center">126.000000</td>
<td align="center">1970.000000</td>
<td align="center">-37.802355</td>
<td align="center">145.000100</td>
<td align="center">6555.000000</td>
</tr>
<tr>
<td align="center">75%</td>
<td align="center">3.000000</td>
<td align="center">1.330000e+06</td>
<td align="center">13.000000</td>
<td align="center">3148.000000</td>
<td align="center">3.000000</td>
<td align="center">2.000000</td>
<td align="center">2.000000</td>
<td align="center">651.000000</td>
<td align="center">174.000000</td>
<td align="center">1999.000000</td>
<td align="center">-37.756400</td>
<td align="center">145.058305</td>
<td align="center">10331.000000</td>
</tr>
<tr>
<td align="center">max</td>
<td align="center">10.000000</td>
<td align="center">9.000000e+06</td>
<td align="center">48.100000</td>
<td align="center">3977.000000</td>
<td align="center">20.000000</td>
<td align="center">8.000000</td>
<td align="center">10.000000</td>
<td align="center">433014.000000</td>
<td align="center">44515.000000</td>
<td align="center">2018.000000</td>
<td align="center">-37.408530</td>
<td align="center">145.526350</td>
<td align="center">21650.000000</td>
</tr>
</tbody>
</table>
<p>这个方法在初步拿到一组数据集的时候还是非常好用的，可以快速的了解到这组数据的大致情况。当然，如果想进行数据趋势及其他方面的分析，还是需要一些图进行数据可视化的。</p>
<h3 id="2">2.第一个模型</h3>
<p>课程尝试引导使用决策树来解决上述问题。首先通过调用columns属性对数据的列进行了观察。</p>
<pre><code class="language-python">import pandas as pd

melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
melbourne_data.columns
</code></pre>
<p>输出：</p>
<pre><code>Index(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',
*    'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',
*    'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',
*    'Longtitude', 'Regionname', 'Propertycount'],
*   dtype='object')

</code></pre>
<p>同时提到部分数据可能具有缺失值，在处理一些具有缺失值的问题时，可以用最简单的删除法处理缺失值。</p>
<pre><code class="language-python">melbourne_data = melbourne_data.dropna(axis=0)
</code></pre>
<p>在选取用来训练的数据子集的时候，课程介绍了点符号和一个由列的名称组成的list的方法进行选取。课程中，使用了点符号选择了预测目标，使用列的名称的list选取了用于训练的特征。然后用describe()方法和head()方法（输出数据的前5条）对选取出来的X（包含特征的数据子集）进行了检查。</p>
<pre><code class="language-python">melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']
y = melbourne_data.Price
X = melbourne_data[melbourne_features]
</code></pre>
<p>在准备好所有所需数据之后，可成就开始调用sklearn.tree中的DecisionTreeRegressor开始的拟合构建第一个模型。</p>
<pre><code class="language-python">from sklearn.tree import DecisionTreeRegressor

# Define model. Specify a number for random_state to ensure same results each run
melbourne_model = DecisionTreeRegressor(random_state=1)

# Fit model
melbourne_model.fit(X, y)
</code></pre>
<h3 id="3">3.模型的验证</h3>
<p>模型的验证是很重要的一部分，毕竟只有有了足够良好的评判标准，才能知道如何让自己设计的模型变得越来越好。</p>
<p>在验证模型的时候，训练集和测试集的区分是很重要的一个事情，因为机器学习存在过拟合这个问题，如果使用训练集作为验证模型是否良好的指标，那么模型可能过拟合了训练集，也就是在训练集上表现的越来越好，直至只在训练集上表现的“完美无缺”，而在真正使用这个模型的实际场景上，则变得很差，这时模型就像一个优秀的“应试型学生”一样，仅仅在如同试卷的训练集上非常优秀，但是在真正的实际应用场景中就变得一塌糊涂。</p>
<p>在kaggle的课程中，先从平均绝对误差(Mean Absolute Error also called MAE)开始介绍，首先聚焦于误差，其中误差等于：
$$误差 = 预测值 - 实际值$$</p>
<p>然后MAE指标就是对于每个误差取绝对值，然后取这些绝对误差的平均值。这就是衡量模型质量的MAE标准。其对于n个数据的公式为
$$ MAE = \frac{\sum_{1}^{n}\left|errro\right|}{n} =\frac{\sum_{1}^{n}\left|\hat{y}-y\right|}{n}$$</p>
<p>在课程中，kaggle先训练了一个“墨尔本房价”的模型，然后调用sklearn.metrics中的mean_absolute_error进行平均绝对误差的验证，具体代码是这样的：</p>
<pre><code class="language-python"># 模型构建
# Data Loading Code Hidden Here
import pandas as pd

# Load data
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
# Filter rows with missing price values
filtered_melbourne_data = melbourne_data.dropna(axis=0)
# Choose target and features
y = filtered_melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
* * * * * * 'YearBuilt', 'Lattitude', 'Longtitude']
X = filtered_melbourne_data[melbourne_features]

from sklearn.tree import DecisionTreeRegressor
# Define model
melbourne_model = DecisionTreeRegressor()
# Fit model
melbourne_model.fit(X, y)
</code></pre>
<pre><code class="language-python"># 平均绝对误差评测
from sklearn.metrics import mean_absolute_error

predicted_home_prices = melbourne_model.predict(X)
mean_absolute_error(y, predicted_home_prices)
</code></pre>
<p>当然前文提到了最好训练集和测试集是分开的，所以课程也介绍了通过调用sklearn.model_selection中train_test_split用于在一个数据集中随机分离训练集和测试集的方法，</p>
<pre><code class="language-python">from sklearn.model_selection import train_test_split

# split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)
# Define model
melbourne_model = DecisionTreeRegressor()
# Fit model
melbourne_model.fit(train_X, train_y)

# get predicted prices on validation data
val_predictions = melbourne_model.predict(val_X)
print(mean_absolute_error(val_y, val_predictions))
</code></pre>
<h3 id="4">4.过拟合和欠拟合</h3>
<p>kaggle用了一章来介绍过拟合和欠拟合的相关知识，我感觉这部分知识属于比较基础的内容，所以就简单描述下。</p>
<p>针对于决策树部分，课程介绍的针对于决策树最主要的解决过拟合的方法就是剪枝，也就是通过控制叶子结点的个数来增强模型的拟合能力。在课程中，列出了一个通过计算不同最大节点数下MAE值的函数来找出最合适的结点树，具体代码如下：</p>
<pre><code class="language-python">from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor

def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
* model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
* model.fit(train_X, train_y)
* preds_val = model.predict(val_X)
* mae = mean_absolute_error(val_y, preds_val)
* return(mae)

for max_leaf_nodes in [5, 50, 500, 5000]:
* my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
* print(&quot;Max leaf nodes: %d  \t\t Mean Absolute Error:  %d&quot; %(max_leaf_nodes, my_mae))
</code></pre>
<h3 id="5">5.随机森林</h3>
<p>在kaggle“认识机器学习”的最后一章中试图介绍了以随进森林算法为代表的更为先进的建模方法。随机森林使用很多棵树，通过平均每棵树的预测结果来进行预测。当然，这部分理论知识比较少，还是仅仅贴出了一份代码，并且通过最后远低于之前单随机树算法的MAE值来证明随机森林的优越性。</p>
<pre><code class="language-python">#见过很多次的数据调用以及划分训练集、测试集。

import pandas as pd
* 
# Load data
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path) 
# Filter rows with missing values
melbourne_data = melbourne_data.dropna(axis=0)
# Choose target and features
y = melbourne_data.Price
melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
* * * * * * 'YearBuilt', 'Lattitude', 'Longtitude']
X = melbourne_data[melbourne_features]

from sklearn.model_selection import train_test_split

# split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
train_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)


</code></pre>
<pre><code class="language-python">#调用随机森林方法并进行训练  

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)
melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
</code></pre>
<h2 id="_2">特征工程</h2>
<p>学习的第二部分我决定了解一下特征工程，因为我去尝试做了一下kaggle的新手区的比赛<a href="https://www.kaggle.com/competitions/titanic">Titanic - Machine Learning from Disaster</a>获得了Score0.75598的优秀成绩:laughing:，当然，我看到了有无数（297个）拿了1的大佬和1w多分数比我高的同志们，我也想要一个高一点的分数。在浏览了好多好多评论之后，我看到了一片看起来相当靠谱的指导帖，在里面提到了要好好做一下特征工程，我想这是我曾经没有关注过的层面，所以我想试试在经过特征工程之后我的模型可以变的多好。</p>
<h3 id="_3">什么是特征工程</h3>
<p>特征工程的目标就是使数据更适合当前的问题，在kaggle特征工程的相关课程里，成将介绍讲解</p>
<ol>
<li>如何确定那些特征是最重要的特征</li>
<li>在实际问题中如何找到/创造新的特征</li>
<li>对于一些复杂问题重新进行编码</li>
<li>使用k-means聚类进行特征分割</li>
<li>使用主成分分析法对数据进行分割成为新的特征</li>
</ol>
<p>通过以上这些方法，特征工程将
* 提高模型的预测性能
* 减少计算或数据需求
* 提高结果的可解释性</p>
<p>特征工程的知道原则是让特征更好的发挥作用，而特征能发挥作用的前提便是特征与我们所要也测的目标之间存在着某些联系。比如线性模型只能学习线性关系，因而在做某些线性相关模型的时候，应该更专注于将特征转化的与目标呈线性关系。这里的关键在于，对特征进行的转换实质上是模型本身的一部分。比如说，想通过一边的长度来预测正方形地块的价格。直接对长度拟合线性模型的结果会很差：因为边长和价格之间不是线性关系。然而，如果我们将长度特征进行平方求得 "面积"，就会创建一个线性关系。将 "面积 "添加到特征集意味着这个线性模型现在可以拟合抛物线。换句话说，将特征平方后，线性模型就能拟合平方特征了。</p>
<p><img alt="仅以长度为特征的线性模型拟合效果不佳。" src="https://storage.googleapis.com/kaggle-media/learn/images/5D1z24N.png" /></p>
<p>仅以长度为特征的线性模型拟合效果不佳。</p>
<p><img alt="左图： 与 &quot;面积 &quot;的契合度更高。右图 长度的拟合效果也更好" src="https://storage.googleapis.com/kaggle-media/learn/images/BLRsYOK.png" /></p>
<p>左图： 与 "面积 "的契合度更高。右图 长度的拟合效果也更好</p>
<p>这也就是为什么特征工程可以对模型有很好的效果提升，当模型不能很好的学习某些特征的时候，可以由特征工程对于特征进行转化提升，让特征更好的适应模型。换句话说，如何让信息更好的适应模型应该是每次开发模型的时候要考虑的一个关键。</p>
<h3 id="_4">特征工程的实践</h3>
<p>kaggle课程上通过一个由混凝土配方预测混凝土抗压强度的背景来体现特征工程的作用。</p>
<p>首先，先在未进行任何增强的数据上进行一次随机森林的模型训练，从而建立一个基线。这将有助于我们确定新特征是否真的有用。在特征工程流程的开始阶段，建立这样的基线是一种很好的做法。基线分数可以帮助你决定新功能是否值得保留，或者是否应该放弃它们并尝试其他方法。</p>
<p>具体代码在此：</p>
<pre><code class="language-python">import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

df = pd.read_csv(&quot;../input/fe-course-data/concrete.csv&quot;)
df.head()


</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Cement</th>
<th align="center">BlastFurnaceSlag</th>
<th align="center">FlyAsh</th>
<th align="center">Water</th>
<th align="center">Superplasticizer</th>
<th align="center">CoarseAggregate</th>
<th align="center">FineAggregate</th>
<th align="center">Age</th>
<th align="center">CompressiveStrength</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">540.0</td>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">162.0</td>
<td align="center">2.5</td>
<td align="center">1040.0</td>
<td align="center">676.0</td>
<td align="center">28</td>
<td align="center">79.99</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">540.0</td>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">162.0</td>
<td align="center">2.5</td>
<td align="center">1055.0</td>
<td align="center">676.0</td>
<td align="center">28</td>
<td align="center">61.89</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">332.5</td>
<td align="center">142.5</td>
<td align="center">0.0</td>
<td align="center">228.0</td>
<td align="center">0.0</td>
<td align="center">932.0</td>
<td align="center">594.0</td>
<td align="center">270</td>
<td align="center">40.27</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">332.5</td>
<td align="center">142.5</td>
<td align="center">0.0</td>
<td align="center">228.0</td>
<td align="center">0.0</td>
<td align="center">932.0</td>
<td align="center">594.0</td>
<td align="center">365</td>
<td align="center">41.05</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">198.6</td>
<td align="center">132.4</td>
<td align="center">0.0</td>
<td align="center">192.0</td>
<td align="center">0.0</td>
<td align="center">978.4</td>
<td align="center">825.5</td>
<td align="center">360</td>
<td align="center">44.30</td>
</tr>
</tbody>
</table>
<pre><code class="language-python">X = df.copy()
y = X.pop(&quot;CompressiveStrength&quot;)

# Train and score baseline model
baseline = RandomForestRegressor(criterion=&quot;absolute_error&quot;, random_state=0)
baseline_score = cross_val_score(
    baseline, X, y, cv=5, scoring=&quot;neg_mean_absolute_error&quot;
)
baseline_score = -1 * baseline_score.mean()

print(f&quot;MAE Baseline Score: {baseline_score:.4}&quot;)


</code></pre>
<blockquote>
<p>MAE Baseline Score: 8.232</p>
</blockquote>
<p>如果在家做过饭，就可能知道食谱中配料的比例通常比配料的绝对量更能预测食谱的结果。因此我们可以推断，上述特征的比率可以很好地预测压缩强度。</p>
<p>下面的代码为数据集添加了三个新的比率特征。（震惊，水泥里有骨粉？？！！！还是deepl翻译错了）（和某颜求证了可能是骨料，粗骨料是石子，细骨料是沙子）</p>
<pre><code class="language-python">

X = df.copy()
y = X.pop(&quot;CompressiveStrength&quot;)

# Create synthetic features
X[&quot;FCRatio&quot;] = X[&quot;FineAggregate&quot;] / X[&quot;CoarseAggregate&quot;] #细骨料与粗骨料的比例
X[&quot;AggCmtRatio&quot;] = (X[&quot;CoarseAggregate&quot;] + X[&quot;FineAggregate&quot;]) / X[&quot;Cement&quot;]#粗细骨料与水泥的比例
X[&quot;WtrCmtRatio&quot;] = X[&quot;Water&quot;] / X[&quot;Cement&quot;]#水和水泥的比例

# Train and score model on dataset with additional ratio features
model = RandomForestRegressor(criterion=&quot;absolute_error&quot;, random_state=0)
score = cross_val_score(
    model, X, y, cv=5, scoring=&quot;neg_mean_absolute_error&quot;
)
score = -1 * score.mean()

print(f&quot;MAE Score with Ratio Features: {score:.4}&quot;)


</code></pre>
<blockquote>
<p>MAE Score with Ratio Features: 7.948</p>
</blockquote>
<p>果然，性能得到了改善！这证明，这些新的比率特征向模型揭示了模型之前没有检测到的重要信息。</p>
<h3 id="mutual-information">互信息(Mutual Information)</h3>
<p>当我们初次接触一个新的数据集，常常会感到不知所措。因为我们可能会看到成百上千个甚至连描述都没有的特征，那我们该从何入手呢？</p>
<p>第一步就是利用特征效用指标（一种衡量特征与目标之间关联的函数）构建一个排名。这样，您就可以选择一部分最有用的特征进行初步开发，暂且忽略掉其他效用不那么好的特征，从而提升开发的效率。</p>
<p>这些被由来衡量特征效用的指标称为 "互信息"。互信息很像相关性，它衡量的是两个量之间的关系。互信息的优势在于它可以检测任何类型的关系，而相关性只能检测线性关系。</p>
<p>互信息是一个很好的通用指标，尤其是在功能开发之初，当你还不知道要使用什么模型时，互信息非常有用。它具有以下优点：</p>
<ul>
<li>易于使用和解释</li>
<li>计算效率高</li>
<li>理论依据充分</li>
<li>不易过度拟合</li>
<li>能够检测任何类型的关系</li>
</ul>
<p>那么接下来我们介绍互信息及它的衡量标准。互信息从不确定性的角度描述关系。两个量之间的互信息（MI）是衡量对一个量的了解在多大程度上减少了另一个量的不确定性。如果您知道某个特征的值，您对目标的信心会增加多少？（没读懂，待会再翻译吧）</p>
<p>kaggle课程给出了艾姆斯房屋数据中的一个例子。图中显示了房屋外观质量与售价之间的关系。每个点代表一栋房子。</p>
<p><img alt="了解房屋的外部质量可以减少售价的不确定性" src="https://storage.googleapis.com/kaggle-media/learn/images/X12ARUK.png" /></p>
<p>了解房屋的外部质量可以减少售价的不确定性</p>
<p>从图中我们可以看出，知道 ExterQual 的值，就能更确定相应的销售价格--每一类 ExterQual 都会将销售价格集中在一定范围内。ExterQual 与 SalePrice 之间的相互信息是四种 ExterQual 值对 SalePrice 不确定性的平均减小程度。举例来说，由于 "Fair "比 "Typical "出现的频率低，因此 "Fair "在互信息评分中的权重较低。</p>
<p>(技术说明：我们所说的不确定性是用信息论中的一个量 "熵 "来衡量的。一个变量的熵大致是指 "你平均需要问多少个'是'或'否'的问题来描述该变量的一次出现"。要问的问题越多，变量的不确定性就越大。互信息是指你期望特征回答多少个关于目标的问题（Mutual information is how many questions you expect the feature to answer about the target)。</p>
<p>解读互信息得分</p>
<p>数量间的最小互信息为 0.0。当 MI 为零时，两个量是独立的：任何一个量都不能告诉你关于另一个量的任何信息。相反，理论上，互信息指数没有上限。但在实践中，超过 2.0 左右的值并不常见。（互信息是一个对数量，因此它的增长速度非常缓慢）。</p>
<p>下图将让你了解 MI 值如何与特征与目标之间的关联类型和程度相对应。</p>
<p><img alt="左图： 随着特征与目标之间的依赖关系越来越紧密，互信息也随之增加。右图 互信息可以捕捉任何类型的关联（不仅仅是线性关联，如相关性。）" src="https://storage.googleapis.com/kaggle-media/learn/images/Dt75E1f.png" /></p>
<p>左图： 随着特征与目标之间的依赖关系越来越紧密，互信息也随之增加。右图 互信息可以捕捉任何类型的关联（不仅仅是线性关联，如相关性。）</p>
<p>在应用互信息时，需要记住以下几点：</p>
<ul>
<li>互信息可以帮助您了解某个特征单独作为目标预测指标的相对潜力。</li>
<li>当一个特征与其他特征相互作用时，它的信息量可能会非常大，但单独使用时信息量就不会太大。MI 无法检测特征之间的交互作用。它是一种单变量度量。</li>
<li>特征的实际有用性取决于与之配合使用的模型。只有当特征与目标之间的关系是模型可以学习的关系时，特征才是有用的。一个特征的 MI 分数很高，但这并不意味着你的模型可以利用这些信息做任何事情。您可能需要先转换特征，以揭示关联。</li>
</ul>
<p>关于互信息的一个示例：1985 年汽车</p>
<p>汽车数据集由 1985 年款的 193 辆汽车组成。该数据集的目标是根据汽车的 23 个特征（如品牌、车身样式和马力）预测汽车的价格（目标值）。在本示例中，我们将利用互信息对特征进行排序，并通过数据可视化研究结果。</p>
<pre><code class="language-python">

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

plt.style.use(&quot;seaborn-whitegrid&quot;)

df = pd.read_csv(&quot;../input/fe-course-data/autos.csv&quot;)
df.head()

</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">symboling</th>
<th align="center">make</th>
<th align="center">fuel_type</th>
<th align="center">aspiration</th>
<th align="center">num_of_doors</th>
<th align="center">body_style</th>
<th align="center">drive_wheels</th>
<th align="center">engine_location</th>
<th align="center">wheel_base</th>
<th align="center">length</th>
<th align="center">...</th>
<th align="center">engine_size</th>
<th align="center">fuel_system</th>
<th align="center">bore</th>
<th align="center">stroke</th>
<th align="center">compression_ratio</th>
<th align="center">horsepower</th>
<th align="center">peak_rpm</th>
<th align="center">city_mpg</th>
<th align="center">highway_mpg</th>
<th align="center">price</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">3</td>
<td align="center">alfa-romero</td>
<td align="center">gas</td>
<td align="center">std</td>
<td align="center">2</td>
<td align="center">convertible</td>
<td align="center">rwd</td>
<td align="center">front</td>
<td align="center">88.6</td>
<td align="center">168.8</td>
<td align="center">...</td>
<td align="center">130</td>
<td align="center">mpfi</td>
<td align="center">3.47</td>
<td align="center">2.68</td>
<td align="center">9</td>
<td align="center">111</td>
<td align="center">5000</td>
<td align="center">21</td>
<td align="center">27</td>
<td align="center">13495</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">3</td>
<td align="center">alfa-romero</td>
<td align="center">gas</td>
<td align="center">std</td>
<td align="center">2</td>
<td align="center">convertible</td>
<td align="center">rwd</td>
<td align="center">front</td>
<td align="center">88.6</td>
<td align="center">168.8</td>
<td align="center">...</td>
<td align="center">130</td>
<td align="center">mpfi</td>
<td align="center">3.47</td>
<td align="center">2.68</td>
<td align="center">9</td>
<td align="center">111</td>
<td align="center">5000</td>
<td align="center">21</td>
<td align="center">27</td>
<td align="center">16500</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">alfa-romero</td>
<td align="center">gas</td>
<td align="center">std</td>
<td align="center">2</td>
<td align="center">hatchback</td>
<td align="center">rwd</td>
<td align="center">front</td>
<td align="center">94.5</td>
<td align="center">171.2</td>
<td align="center">...</td>
<td align="center">152</td>
<td align="center">mpfi</td>
<td align="center">2.68</td>
<td align="center">3.47</td>
<td align="center">9</td>
<td align="center">154</td>
<td align="center">5000</td>
<td align="center">19</td>
<td align="center">26</td>
<td align="center">16500</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">audi</td>
<td align="center">gas</td>
<td align="center">std</td>
<td align="center">4</td>
<td align="center">sedan</td>
<td align="center">fwd</td>
<td align="center">front</td>
<td align="center">99.8</td>
<td align="center">176.6</td>
<td align="center">...</td>
<td align="center">109</td>
<td align="center">mpfi</td>
<td align="center">3.19</td>
<td align="center">3.40</td>
<td align="center">10</td>
<td align="center">102</td>
<td align="center">5500</td>
<td align="center">24</td>
<td align="center">30</td>
<td align="center">13950</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">2</td>
<td align="center">audi</td>
<td align="center">gas</td>
<td align="center">std</td>
<td align="center">4</td>
<td align="center">sedan</td>
<td align="center">4wd</td>
<td align="center">front</td>
<td align="center">99.4</td>
<td align="center">176.6</td>
<td align="center">...</td>
<td align="center">136</td>
<td align="center">mpfi</td>
<td align="center">3.19</td>
<td align="center">3.40</td>
<td align="center">8</td>
<td align="center">115</td>
<td align="center">5500</td>
<td align="center">18</td>
<td align="center">22</td>
<td align="center">17450</td>
</tr>
</tbody>
</table>
<p>用于 MI 的 scikit-learn 算法对离散特征和连续特征的处理方式不同。因此，你需要告诉它哪些是离散特征。根据经验，任何必须使用浮点类型的特征都不是离散特征。分类（对象或分类 d 类型）可以通过赋予标签编码来视为离散特征。（您可以在我们的分类变量课程中复习标签编码）。</p>
<pre><code class="language-python">

X = df.copy()
y = X.pop(&quot;price&quot;)

# Label encoding for categoricals
for colname in X.select_dtypes(&quot;object&quot;):
    X[colname], _ = X[colname].factorize()

# All discrete features should now have integer dtypes (double-check this before using MI!)
discrete_features = X.dtypes == int


</code></pre>
<p>Scikit-learn 的 feature_selection 模块中有两个互信息度量：一个用于实值目标（mutual_info_regression），另一个用于分类目标（mutual_info_classif）。我们的目标价格是实值目标。下一个单元格将计算特征的 MI 分数，并将其封装在一个漂亮的数据框架中。</p>
<pre><code class="language-python">from sklearn.feature_selection import mutual_info_regression

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name=&quot;MI Scores&quot;, index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

mi_scores = make_mi_scores(X, y, discrete_features)
mi_scores[::3]  # show a few features with their MI scores
</code></pre>
<blockquote>
<p>curb_weight          1.540126<br />
highway_mpg          0.951700<br />
length               0.621566<br />
fuel_system          0.485085<br />
stroke               0.389321<br />
num_of_cylinders     0.330988<br />
compression_ratio    0.133927<br />
fuel_type            0.048139<br />
Name: MI Scores, dtype: float64</p>
</blockquote>
<p>现在是条形图，便于比较：</p>
<pre><code class="language-python">

def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title(&quot;Mutual Information Scores&quot;)


plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)


</code></pre>
<p><img alt="" src="https://www.kaggleusercontent.com/kf/126574300/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..bllHzVuyXlIRp5UBeS7xxg.ZL81lpx0tXLHUfYA1guSyTHfANtYMy9c2AvYtsxFZL0CGR-LEFBLrTdib_cOyMCpLQE_dIKzCQtyKYx5yS4cEHaH2gIubOS1rN3FfRyb6iw9V3HyUiCZAtt5SmlVSpzKjsYckEpFWZrOIs2LXpcgRAxd5qJ08aQ51zquXJuAMvJyJXltQ85sS4fA-IkoLq1H0lvrf9C61P2-0oJh_MC0PP0Xi3lZYZyOG5ryBx5NYokkoC7pgPjwKZIqp64p-kNXHu0eJ3gVcT-rtTgLTUezH06shczaws_npH6BWu9vtB9Y5LK1CEpgVp0n3WiififP6gnBa9iYgPGTsfcqSLIGCftHOipMt0RNs-oJRk08y1qm9SrvkkZXohJBe0ZJtil5qnCi2pPCCDLzJJt06k3ZKrDunbPfyVJjDt2aidIhpZa-0v-aP2zxllkZTz374dWHvFdH42hJm61tK9ylZRtqjn6Hp3D5NevxcMGevVUr46NVeg3Y29QM64rTMsyrpbng420VRSarZ1OsmoM1PWbBI7LEoopYwUBXb9ENkmCRecReeSPecTZWv4fSSQ-JPZjO6HRGR8--NNMquivNQYoxq4ZehoZoaLLLwxiM83eKk0TeNgOrMfX18QJdUWs6p2_8VVWxPgsqeCHTiuQYK2v-LGGESWgtZW0prcf51qj2xGU.yJtlDrFK5LMKA7PAnXT1Fw/__results___files/__results___7_0.png" /></p>
<p>数据可视化是实用工具排名的重要后续手段。让我们仔细看看其中的几个。正如我们所预料的那样， curb_weight 特征获得了最高分，表明着它与我们的目标“价格”关系密切。</p>
<pre><code class="language-python">sns.relplot(x=&quot;curb_weight&quot;, y=&quot;price&quot;, data=df);
</code></pre>
<p><img alt="" src="https://www.kaggleusercontent.com/kf/126574300/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mEtfsAZUy-oKHAAmHaLokQ.bC08NCVVyM1ImVjt1J4-PpIbiIqihcn0VntRg-9lKtLzKVT-QkDBQp2vB_KwZFA3mCXKvB3v-ByyMo9N-932n_LMJKKPfwjiCh2EilXhul9gLam9DVeb5MCUvSMARGteVVhGjd13DIW4fFTl6PBy4aqKfNpOF8hN5JCgy0qycN15kj6N3VVdlYZQAIYYI5XbeYOYP0-dru02xaZgiDlYWuYScuYl20NJ6zF6SBNrrYTyoY1zYy1hnLP8LV14LrtMnHdBlZRfp6tkhh3VxhctvpIk1yyk0FntC9kUqaz4xKr-ii7BB30SDF-Eln95sI-UDi-VR62pcKAmf0FmhQGozuq8JLB_kkStehwGjt26ppiuN8AZcRFyPkGO2vIKVhyyTrtMqtxgoM2cnuleKukERCn6HWMaI1LUf3SNoWyzLZu9c09wbz5QyalJBGFP4BE9i9q5qzTtpg0lnK9i0NCxgoAzSdZTycWW51hy9nTuBRX8CxK48CEoC2cRcGbnUfAqK1QTlBSz_62kzeogq0bkq7DneaOhf1maDWNkd5WUHpJb-Agz0rUHwkBfLzeXmhFKxy5G6GF5CXKSdi9iK4G-vCozx4OLzuergU2CiYSxn24mUlRcnU9GaHD2q0sdG3pT9hUqGaab0UYZnDZINRHPN3ijEc_bvN-uO6xDxdho18w.hRDtM4DtSUL_Ia0S7TsXOA/__results___files/__results___9_0.png" /></p>
<p>燃料类型特征的 MI 分数相当低，但我们可以从图中看到，它明显区分了马力特征中两个趋势不同的价格群体。这表明燃料类型产生了交互效应，可能并非不重要。在根据 MI 分数判定某个特征不重要之前，最好先研究一下任何可能的交互效应——相关的专业知识在这方面可以提供很多指导。</p>
<p><img alt="" src="https://www.kaggleusercontent.com/kf/126574300/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..mEtfsAZUy-oKHAAmHaLokQ.bC08NCVVyM1ImVjt1J4-PpIbiIqihcn0VntRg-9lKtLzKVT-QkDBQp2vB_KwZFA3mCXKvB3v-ByyMo9N-932n_LMJKKPfwjiCh2EilXhul9gLam9DVeb5MCUvSMARGteVVhGjd13DIW4fFTl6PBy4aqKfNpOF8hN5JCgy0qycN15kj6N3VVdlYZQAIYYI5XbeYOYP0-dru02xaZgiDlYWuYScuYl20NJ6zF6SBNrrYTyoY1zYy1hnLP8LV14LrtMnHdBlZRfp6tkhh3VxhctvpIk1yyk0FntC9kUqaz4xKr-ii7BB30SDF-Eln95sI-UDi-VR62pcKAmf0FmhQGozuq8JLB_kkStehwGjt26ppiuN8AZcRFyPkGO2vIKVhyyTrtMqtxgoM2cnuleKukERCn6HWMaI1LUf3SNoWyzLZu9c09wbz5QyalJBGFP4BE9i9q5qzTtpg0lnK9i0NCxgoAzSdZTycWW51hy9nTuBRX8CxK48CEoC2cRcGbnUfAqK1QTlBSz_62kzeogq0bkq7DneaOhf1maDWNkd5WUHpJb-Agz0rUHwkBfLzeXmhFKxy5G6GF5CXKSdi9iK4G-vCozx4OLzuergU2CiYSxn24mUlRcnU9GaHD2q0sdG3pT9hUqGaab0UYZnDZINRHPN3ijEc_bvN-uO6xDxdho18w.hRDtM4DtSUL_Ia0S7TsXOA/__results___files/__results___11_0.png" /></p>
<p>数据可视化是功能工程工具箱的重要补充。除了互信息等实用指标外，可视化还能帮助您发现数据中的重要关系。</p>
<h3 id="_5">特征的开发</h3>
<p>在探究选择出一组具有潜力的特征之后，就可以对这些特征进行开发，来创造最适合模型的特征。在kaggle课程中将使用以下四个数据集进行讲解：美国交通事故、1985 年汽车、混凝土配方和客户终身价值。</p>
<p>在开发特征的时候，有以下一下小tips是可以关注的：
* <strong>了解特征</strong>：参考数据集的数据文档（如果有的话）。
*  <strong>研究问题领域，获取领域知识</strong>：如果您的问题是预测房价，那么可以做一些房地产方面的研究。维基百科可能是一个很好的起点，但书籍和期刊论文通常能提供最好的信息。
* <strong>研究以前的工作</strong>：过去 Kaggle 竞赛中的解决方案文章是很好的资源。
* <strong>使用数据可视化</strong>：可视化可以揭示特征分布中的病理或可以简化的复杂关系。在特征工程设计过程中，一定要将数据集可视化。</p>
<h4 id="_6">数字型特征开发</h4>
<p>数字类型特征之间的关系通常通过数学公式来表达，这是经常会遇到的。一般在pandas中对列进行算术运算，就像对普通数字一样。</p>
<p>在汽车数据集中有描述汽车发动机的特征。通过研究可以获得各种公式，用于创建可能有用的新特征。例如，"冲程比 "可以衡量发动机的效率和性能：</p>
<pre><code class="language-python">autos[&quot;stroke_ratio&quot;] = autos.stroke / autos.bore

autos[[&quot;stroke&quot;, &quot;bore&quot;, &quot;stroke_ratio&quot;]].head()
</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">stroke</th>
<th align="center">bore</th>
<th align="center">stroke_ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">2.68</td>
<td align="center">3.47</td>
<td align="center">0.772334</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">2.68</td>
<td align="center">3.47</td>
<td align="center">0.772334</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">3.47</td>
<td align="center">2.68</td>
<td align="center">1.294776</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">3.40</td>
<td align="center">3.19</td>
<td align="center">1.065831</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">3.40</td>
<td align="center">3.19</td>
<td align="center">1.065831</td>
</tr>
</tbody>
</table>
<p>组合越复杂，模型学习起来就越困难，就像发动机 "排量 "的计算公式，它是衡量发动机功率的标准：</p>
<pre><code class="language-python">autos[&quot;displacement&quot;] = (
    np.pi * ((0.5 * autos.bore) ** 2) * autos.stroke * autos.num_of_cylinders
)

</code></pre>
<p>数据可视化可以提出转换建议，通常是通过幂或对数对特征进行 "重塑"。例如，美国事故中风速的分布高度倾斜。在这种情况下，对数可以有效地将其归一化：</p>
<pre><code class="language-python"># If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log
accidents[&quot;LogWindSpeed&quot;] = accidents.WindSpeed.apply(np.log1p)

# Plot a comparison
fig, axs = plt.subplots(1, 2, figsize=(8, 4))
sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])
sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1])
</code></pre>
<p><img alt="" src="https://www.kaggleusercontent.com/kf/126574294/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..p6wnDP2RtDMATIOYR0XS0w.nZA4HlW_PWRmy9b9pFGY9P61KZxoHRIHBIFHrrEOOkgzR6WsqeOo_Oym23RwkO92EjtH6f9UkAlCHHvz2hDB-ibgPzEbORjP0DoHl8z7cwS6fc1E5PvPO2qxUMfoMzdrwuAO1o9ZheU66Kf13J7Lkfri0zMA32yvJDZnwKGA_31XhPSWZGftYaoQJgKbU9xqJYOz1DrekhvInTT0GL9UqoCRl2vbd8sGPPfq36lNgGl0xvVAdQlESPN71YH47JcJJlCw5BEWGOBhdxaqXeA6lqEArx74qtcmIviYlkY7cVOew6YG9IkoHuRjmZ7JAz-3jnqP0wseC8NqfIzMePuyU9kusNdp9IRQC2NuAd4ubBGpY-Oz_8BAcz1gnTGfXbK90dSDbUO3lYV_0yNGSZD-vCPXHhsYdsW_Oim3lOErx0ZpOjy5yZ_QWJWef-kzR4gCFWmLjgrJT5El2Hi3hn-c3fhZIBJTskIkioPfCoaKbrqu4QRj0nIsqvL9-vObYpM7Hl-SYLNzZM-PgcRXbNC5agvvxOzu0pxc8RhYkROSPV02vLZWCQRC6Mlx0p0GgMoyOSpUMB5_Y7F3C78ANh-6tgtCoPu3Cz9HbHDFN-lga-r2WelkLWpfetpxIdG6mA8MpbhqxtQX8LiyKcBD3yH_b324sNxfobfVi2JPAXGRuNE.QiuvI_Yt_Y3RyZz0i5FIkA/__results___files/__results___7_1.png" /></p>
<h4 id="_7">计数型特征</h4>
<p>描述某种事物存在或不存在的特征经常会再数据中出现，例如某些因素的是否存在会不会引发某种疾病。这些特征一般可以通过加和来创建出一个积极或者消极特征的总数。</p>
<p>这些特征将一般是是二进制的（1 表示存在，0 表示不存在）或布尔型的（真或假）。在 Python 中，布尔值可以像整数一样相加。</p>
<p>在 "交通事故"（Traffic Accidents）中，有几个特征表示事故附近是否有道路物体。这将使用求和方法创建附近道路特征总数的计数：</p>
<pre><code class="language-python">roadway_features = [&quot;Amenity&quot;, &quot;Bump&quot;, &quot;Crossing&quot;, &quot;GiveWay&quot;,
    &quot;Junction&quot;, &quot;NoExit&quot;, &quot;Railway&quot;, &quot;Roundabout&quot;, &quot;Station&quot;, &quot;Stop&quot;,
    &quot;TrafficCalming&quot;, &quot;TrafficSignal&quot;]
accidents[&quot;RoadwayFeatures&quot;] = accidents[roadway_features].sum(axis=1)

accidents[roadway_features + [&quot;RoadwayFeatures&quot;]].head(10)
</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Amenity</th>
<th align="center">Bump</th>
<th align="center">Crossing</th>
<th align="center">GiveWay</th>
<th align="center">Junction</th>
<th align="center">NoExit</th>
<th align="center">Railway</th>
<th align="center">Roundabout</th>
<th align="center">Station</th>
<th align="center">Stop</th>
<th align="center">TrafficCalming</th>
<th align="center">TrafficSignal</th>
<th align="center">RoadwayFeatures</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">True</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">True</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">True</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">True</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">True</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">9</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">False</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>您也可以使用dataframe内置的方法来处理布尔值。在 "混凝土"数据集中有具体配方中的成分数量。许多混凝土配方中缺少一种或多种成分（即成分值为 0）。我们将用dataframe内置的gt方法来计算配方中所含不同成分的数量。</p>
<blockquote>
<p>ps：gt()方法返回与原始序列或数据帧相同形状的布尔值序列或数据帧，表示每个元素是否大于指定的值。
举例：
```python
import pandas as pd
data = {'A': [1, 2, 3, 4, 5],
       'B': [6, 7, 8, 9, 10]}
df = pd.DataFrame(data)</p>
<p>result = df['A'].gt(3)</p>
<p>print(result)</p>
<p>```</p>
<blockquote>
<p>0    False<br />
 1    False<br />
 2    False<br />
 3     True<br />
 4     True<br />
 Name: A, dtype: bool  </p>
</blockquote>
</blockquote>
<pre><code class="language-python">
components = [ &quot;Cement&quot;, &quot;BlastFurnaceSlag&quot;, &quot;FlyAsh&quot;, &quot;Water&quot;,
               &quot;Superplasticizer&quot;, &quot;CoarseAggregate&quot;, &quot;FineAggregate&quot;]
concrete[&quot;Components&quot;] = concrete[components].gt(0).sum(axis=1)

concrete[components + [&quot;Components&quot;]].head(10)

</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Cement</th>
<th align="center">BlastFurnaceSlag</th>
<th align="center">FlyAsh</th>
<th align="center">Water</th>
<th align="center">Superplasticizer</th>
<th align="center">CoarseAggregate</th>
<th align="center">FineAggregate</th>
<th align="center">Components</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">540.0</td>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">162.0</td>
<td align="center">2.5</td>
<td align="center">1040.0</td>
<td align="center">676.0</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">540.0</td>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">162.0</td>
<td align="center">2.5</td>
<td align="center">1055.0</td>
<td align="center">676.0</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">332.5</td>
<td align="center">142.5</td>
<td align="center">0.0</td>
<td align="center">228.0</td>
<td align="center">0.0</td>
<td align="center">932.0</td>
<td align="center">594.0</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">332.5</td>
<td align="center">142.5</td>
<td align="center">0.0</td>
<td align="center">228.0</td>
<td align="center">0.0</td>
<td align="center">932.0</td>
<td align="center">594.0</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">198.6</td>
<td align="center">132.4</td>
<td align="center">0.0</td>
<td align="center">192.0</td>
<td align="center">0.0</td>
<td align="center">978.4</td>
<td align="center">825.5</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">266.0</td>
<td align="center">114.0</td>
<td align="center">0.0</td>
<td align="center">228.0</td>
<td align="center">0.0</td>
<td align="center">932.0</td>
<td align="center">670.0</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">380.0</td>
<td align="center">95.0</td>
<td align="center">0.0</td>
<td align="center">228.0</td>
<td align="center">0.0</td>
<td align="center">932.0</td>
<td align="center">594.0</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">380.0</td>
<td align="center">95.0</td>
<td align="center">0.0</td>
<td align="center">228.0</td>
<td align="center">0.0</td>
<td align="center">932.0</td>
<td align="center">594.0</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">266.0</td>
<td align="center">114.0</td>
<td align="center">0.0</td>
<td align="center">228.0</td>
<td align="center">0.0</td>
<td align="center">932.0</td>
<td align="center">670.0</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">9</td>
<td align="center">475.0</td>
<td align="center">0.0</td>
<td align="center">0.0</td>
<td align="center">228.0</td>
<td align="center">0.0</td>
<td align="center">932.0</td>
<td align="center">594.0</td>
<td align="center">4</td>
</tr>
</tbody>
</table>
<h4 id="_8">分解组合特征</h4>
<p>通常情况下，复杂的字符串可以分解成较简单的部分。一些常见的例子</p>
<blockquote>
<p>ID 号码： '123-45-6789'
电话号码： '(999) 555-0123'
街道地址："8241 Kaggle Ln., Goose City, NV
互联网地址：'http://www.kaggle.com
产品代码："0 36000 29145 2
日期和时间："Mon Sep 30 07:06:05 2013</p>
</blockquote>
<p>类似这样的功能通常都有某种规律可以加以利用。例如，美国电话号码有一个区号（"(999) "部分），可以告诉您来电者的位置。和往常一样，在这里进行一些研究会有所收获。</p>
<p>使用 str 访问器，可以直接将 split 等字符串方法应用到列中。 Customer Lifetime Value数据集中描述了保险公司客户的一些特征。我们可以从Policy 特征中分离出不同承保等级及承保类型：</p>
<pre><code class="language-python">

customer[[&quot;Type&quot;, &quot;Level&quot;]] = (  # Create two new features
    customer[&quot;Policy&quot;]           # from the Policy feature
    .str                         # through the string accessor
    .split(&quot; &quot;, expand=True)     # by splitting on &quot; &quot;
                                 # and expanding the result into separate columns
)

customer[[&quot;Policy&quot;, &quot;Type&quot;, &quot;Level&quot;]].head(10)


</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Policy</th>
<th align="center">Type</th>
<th align="center">Level</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">Corporate L3</td>
<td align="center">Corporate</td>
<td align="center">L3</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">Personal L3</td>
<td align="center">Personal</td>
<td align="center">L3</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">Personal L3</td>
<td align="center">Personal</td>
<td align="center">L3</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">Corporate L2</td>
<td align="center">Corporate</td>
<td align="center">L2</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">Personal L1</td>
<td align="center">Personal</td>
<td align="center">L1</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">Personal L3</td>
<td align="center">Personal</td>
<td align="center">L3</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">Corporate L3</td>
<td align="center">Corporate</td>
<td align="center">L3</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">Corporate L3</td>
<td align="center">Corporate</td>
<td align="center">L3</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">Corporate L3</td>
<td align="center">Corporate</td>
<td align="center">L3</td>
</tr>
<tr>
<td align="center">9</td>
<td align="center">Special L2</td>
<td align="center">Special</td>
<td align="center">L2</td>
</tr>
</tbody>
</table>
<p>如果有理由相信特征的组合中存在某种交互作用，也可以将简单的特征连接成一个新的特征（不过说实话我们想到有什么用，所以这块先不写了）</p>
<h4 id="_9">分组变换</h4>
<p>最后，我们还有分组变换，它可以按某种类别进行分组，然后汇总多行信息。通过分组转换，您可以创建以下功能： "一个人居住州的平均收入 "或 "按类型划分的工作日上映电影的比例"。如果你发现了一个类别的交互作用，那么对该类别进行分组变换可能是一个很好的研究方向。</p>
<p>使用聚合函数，分组变换结合了两个特征：一个是提供分组的分类特征，另一个是您希望聚合其值的特征。对于 "各州平均收入"，您可以选择州作为分组特征，平均值作为聚合函数，收入作为聚合特征。要在 Pandas 中计算，我们需要使用 groupby 和 transform 方法：</p>
<pre><code class="language-python">customer[&quot;AverageIncome&quot;] = (
    customer.groupby(&quot;State&quot;)  # for each state
    [&quot;Income&quot;]                 # select the income
    .transform(&quot;mean&quot;)         # and compute its mean
)

customer[[&quot;State&quot;, &quot;Income&quot;, &quot;AverageIncome&quot;]].head(10)
</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">State</th>
<th align="center">Income</th>
<th align="center">AverageIncome</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">Washington</td>
<td align="center">56274</td>
<td align="center">38122.733083</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">Arizona</td>
<td align="center">0</td>
<td align="center">37405.402231</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">Nevada</td>
<td align="center">48767</td>
<td align="center">38369.605442</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">California</td>
<td align="center">0</td>
<td align="center">37558.946667</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">Washington</td>
<td align="center">43836</td>
<td align="center">38122.733083</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">Oregon</td>
<td align="center">62902</td>
<td align="center">37557.283353</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">Oregon</td>
<td align="center">55350</td>
<td align="center">37557.283353</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">Arizona</td>
<td align="center">0</td>
<td align="center">37405.402231</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">Oregon</td>
<td align="center">14072</td>
<td align="center">37557.283353</td>
</tr>
<tr>
<td align="center">9</td>
<td align="center">Oregon</td>
<td align="center">28812</td>
<td align="center">37557.283353</td>
</tr>
</tbody>
</table>
<p>mean()函数是一种dataframe 内置的方法，这意味着我们可以将它作为字符串传递给转换器。其他方便的方法包括 max、min、median、var、std 和 count。下面是计算数据集中每种状态出现频率的方法：</p>
<pre><code class="language-python">customer[&quot;StateFreq&quot;] = (
    customer.groupby(&quot;State&quot;)
    [&quot;State&quot;]
    .transform(&quot;count&quot;)
    / customer.State.count()
)

customer[[&quot;State&quot;, &quot;StateFreq&quot;]].head(10)
</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">State</th>
<th align="center">StateFreq</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">Washington</td>
<td align="center">0.087366</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">Arizona</td>
<td align="center">0.186446</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">Nevada</td>
<td align="center">0.096562</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">California</td>
<td align="center">0.344865</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">Washington</td>
<td align="center">0.087366</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">Oregon</td>
<td align="center">0.284760</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">Oregon</td>
<td align="center">0.284760</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">Arizona</td>
<td align="center">0.186446</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">Oregon</td>
<td align="center">0.284760</td>
</tr>
<tr>
<td align="center">9</td>
<td align="center">Oregon</td>
<td align="center">0.284760</td>
</tr>
</tbody>
</table>
<p>您可以使用这样的转换为分类特征创建 "频率编码"。</p>
<p>如果要使用训练集和验证集，为了保持它们的独立性，最好只使用训练集创建分组特征，然后将其与验证集合并。我们可以在训练集上使用 drop_duplicates 创建一组唯一值后，使用验证集的合并方法：</p>
<pre><code class="language-python"># Create splits
df_train = customer.sample(frac=0.5)
df_valid = customer.drop(df_train.index)

# Create the average claim amount by coverage type, on the training set
df_train[&quot;AverageClaim&quot;] = df_train.groupby(&quot;Coverage&quot;)[&quot;ClaimAmount&quot;].transform(&quot;mean&quot;)

# Merge the values into the validation set
# 关于合并merge()方法可以看下https://baijiahao.baidu.com/s?id=1773445677318349937&amp;wfr=spider&amp;for=pc
df_valid = df_valid.merge(
    df_train[[&quot;Coverage&quot;, &quot;AverageClaim&quot;]].drop_duplicates(),
    on=&quot;Coverage&quot;,
    how=&quot;left&quot;,
)

df_valid[[&quot;Coverage&quot;, &quot;AverageClaim&quot;]].head(10)
</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">Coverage</th>
<th align="center">AverageClaim</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">Premium</td>
<td align="center">671.603973</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">Basic</td>
<td align="center">375.455516</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">Basic</td>
<td align="center">375.455516</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">Basic</td>
<td align="center">375.455516</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">Basic</td>
<td align="center">375.455516</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">Basic</td>
<td align="center">375.455516</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">Basic</td>
<td align="center">375.455516</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">Basic</td>
<td align="center">375.455516</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">Extended</td>
<td align="center">474.483232</td>
</tr>
<tr>
<td align="center">9</td>
<td align="center">Basic</td>
<td align="center">375.455516</td>
</tr>
</tbody>
</table>
<p>在创建功能时，最好牢记模型自身的优缺点。以下是一些创建特征的提示:：</p>
<ol>
<li>线性模型可以自然地学习和与差，但无法学习更复杂的内容。</li>
<li>对于大多数模型来说，比率似乎很难学习。比率组合通常能轻松提高性能。</li>
<li>线性模型和神经网络通常在使用归一化特征时表现更好。基于树的模型（如随机森林和 XGBoost）有时也能从归一化中获益，但通常要少得多。</li>
<li>树状模型可以学习近似几乎所有的特征组合，但当某个组合特别重要时，它们仍然可以从明确创建该组合中获益，尤其是在数据有限的情况下。</li>
<li>计数对树状模型尤其有帮助，因为这些模型没有一种自然的方法来同时汇总许多特征的信息。</li>
</ol>
<h3 id="k-means">以K-means算法为代表的聚类算法</h3>
<p>接下来的两章将使用所谓的无监督学习算法。无监督算法不使用目标；相反，它们的目的是学习数据的某些属性，以某种方式表示特征的结构。在用于预测的特征工程中，你可以将无监督算法视为一种 "特征发现"技术。</p>
<p>聚类简单地说就是根据数据点之间的相似程度，将数据点分配到不同的组中。聚类算法可以说是 "物以类聚"。</p>
<p>当用于特征工程时，我们可以尝试发现代表细分市场的客户群体，或者具有相似天气模式的地理区域。添加聚类标签特征可以帮助机器学习模型理清复杂的空间或邻近关系。</p>
<p>作为特征的聚类标签</p>
<p>对于某个真是存在的特征，聚类的作用类似于传统的 "分选 "或 "离散化 "变换。对于多个特征，它就像 "多维分选"（有时也称为向量量化）。</p>
<p><img alt="" src="https://storage.googleapis.com/kaggle-media/learn/images/sr3pdYI.png" /></p>
<p>左侧为单特征聚类，右侧为两个特征的聚类</p>
<p>将聚类结果加入到dataframe</p>
<table>
<thead>
<tr>
<th align="center">Longitude</th>
<th align="center">Latitude</th>
<th align="center">Cluster</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">-93.619</td>
<td align="center">42.054</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">-93.619</td>
<td align="center">42.053</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">-93.638</td>
<td align="center">42.060</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">-93.602</td>
<td align="center">41.988</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<p>重要的是要记住，这个聚类特征是分类特征。这里显示的是典型聚类算法会产生的标签编码（即整数序列）；根据您的模型，one-hot编码可能更合适。</p>
<p>添加聚类标签的原因是，聚类将把特征间的复杂关系分解成更简单的块。这样，我们的模型就可以逐一学习较简单的部分，而不必一次性学习复杂的整体。这是一种 "分而治之"的策略。</p>
<p><img alt="" src="https://storage.googleapis.com/kaggle-media/learn/images/rraXFed.png" /></p>
<p>对 "建造年份 "特征进行聚类有助于该线性模型了解其与 "销售价格 "之间的关系。</p>
<p>该图显示了聚类如何改进简单的线性模型。对于这种模型来说，"建造年份 "和 "销售价格 "之间的曲线关系过于复杂--不够理想。然而，在较小的块上，这种关系几乎是线性的，因此模型可以很容易地学习</p>
<h4 id="k-means_1">K-means算法介绍</h4>
<p>聚类算法有很多。它们的主要区别在于如何衡量 "相似性 "或 "接近性"，以及使用何种特征。我们将使用的 k-means 算法非常直观，易于在特征工程中应用。当然，在不同的应用情况下，其他的算法也可能更使用。</p>
<p>K-means 聚类使用普通的直线距离（即欧氏距离）来衡量相似性。它通过在特征空间内放置一些称为中心点的点来创建聚类。数据集中的每个点都会被分配到与之最接近的中心点所在的聚类中。k-means "中的 "k "表示创建的中心点（即聚类）的数量。k由你自己定义。</p>
<p>你可以想象每个中心点通过一连串辐射圆来捕捉点。当来自相互竞争的中心点的圆组重叠时，就会形成一条线。这就是所谓的沃罗诺网状结构。该方格网显示了未来数据将被分配到哪些聚类；该方格网本质上就是 k-means 从其训练数据中学习到的。</p>
<p>Ames数据集的聚类就是 k-means 聚类。下面是同样的图，显示了聚类网格和中心点。</p>
<p><img alt="" src="https://storage.googleapis.com/kaggle-media/learn/images/KSoLd3o.jpg" /></p>
<p>K-means 聚类特征即网格呈现的特征空间</p>
<p>让我们回顾一下 k-means 算法是如何学习聚类的，以及这对特征工程意味着什么。我们将重点关注 scikit-learn 实现中的三个参数：n_clusters、max_iter 和 n_init。</p>
<p>这是一个简单的两步过程。算法首先随机初始化一定数量（n_clusters）的中心点。然后迭代这两个操作：</p>
<ul>
<li>将一个数据点分配给最近的聚类中心点</li>
<li>移动每个中心点，以最小化与其点之间的距离
<img alt="" src="https://storage.googleapis.com/kaggle-media/learn/images/tBkCqXJ.gif" /></li>
</ul>
<p>The K-means clustering algorithm on Airbnb rentals in NYC. </p>
<p>如果数据集数量较多，可能需要增加 max_iter，如果数据集较复杂，可能需要增加 n_init。但通常情况下，你唯一需要自己选择的参数是 n_clusters（即 k）。一组特征的最佳划分取决于你正在使用的模型和你试图预测的内容，因此最好像调整其他超参数一样调整它（比如通过交叉验证）。</p>
<p>示例 - 加利福尼亚住房</p>
<p>作为空间特征，加州住房的 "纬度 "和 "经度 "是 K 均值聚类的自然候选特征。在本示例中，我们将这些特征与 "MedInc"（收入中位数）进行聚类，以创建加州不同地区的经济细分。</p>
<pre><code class="language-python">import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans

plt.style.use(&quot;seaborn-whitegrid&quot;)
plt.rc(&quot;figure&quot;, autolayout=True)
plt.rc(
    &quot;axes&quot;,
    labelweight=&quot;bold&quot;,
    labelsize=&quot;large&quot;,
    titleweight=&quot;bold&quot;,
    titlesize=14,
    titlepad=10,
)

df = pd.read_csv(&quot;../input/fe-course-data/housing.csv&quot;)
X = df.loc[:, [&quot;MedInc&quot;, &quot;Latitude&quot;, &quot;Longitude&quot;]]
X.head()

</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">MedInc</th>
<th align="center">Latitude</th>
<th align="center">Longitude</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">8.3252</td>
<td align="center">37.88</td>
<td align="center">-122.23</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">8.3014</td>
<td align="center">37.86</td>
<td align="center">-122.22</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">7.2574</td>
<td align="center">37.85</td>
<td align="center">-122.24</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">5.6431</td>
<td align="center">37.85</td>
<td align="center">-122.25</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">3.8462</td>
<td align="center">37.85</td>
<td align="center">-122.25</td>
</tr>
</tbody>
</table>
<p>由于 k-means 聚类对尺度很敏感，因此最好对具有极端值的数据进行重新尺度化或归一化。我们的特征已经大致在同一尺度上，因此我们将保持不变。</p>
<pre><code class="language-python"># Create cluster feature
kmeans = KMeans(n_clusters=6)
X[&quot;Cluster&quot;] = kmeans.fit_predict(X)
X[&quot;Cluster&quot;] = X[&quot;Cluster&quot;].astype(&quot;category&quot;)

X.head()
</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">MedInc</th>
<th align="center">Latitude</th>
<th align="center">Longitude</th>
<th align="center">Cluster</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">8.3252</td>
<td align="center">37.88</td>
<td align="center">-122.23</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">8.3014</td>
<td align="center">37.86</td>
<td align="center">-122.22</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">7.2574</td>
<td align="center">37.85</td>
<td align="center">-122.24</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">5.6431</td>
<td align="center">37.85</td>
<td align="center">-122.25</td>
<td align="center">5</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">3.8462</td>
<td align="center">37.85</td>
<td align="center">-122.25</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>现在让我们来看几幅图，看看效果如何。首先，散点图显示了聚类的地理分布。算法似乎为沿海高收入地区创建了单独的部分。</p>
<pre><code class="language-python">sns.relplot(
    x=&quot;Longitude&quot;, y=&quot;Latitude&quot;, hue=&quot;Cluster&quot;, data=X, height=6,
);
</code></pre>
<p><img alt="" src="https://www.kaggleusercontent.com/kf/137493070/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..YBmUcOt20Y4K741GbLNClA.-BeHu4IWJE8hEGaZ0nXuD6ukT7Wz9pGfs7q3MTruA4Ay9vZwfdhOQncO2gyoWqB2gQUHUQkDysom82DUuCY_zWU0GCV2Go_XxKjORb2m4l-bbq000WIlltNIOm25HEfeZ56-NhRn18YP57pILuNfRn5fZz3_-RV5iDERJXVBr1I7kORqvpJKMg-hvEuKP5ZdAv5BWkidu7ZMwhEutIbnPm-AYDQmh1nk8kssmY1GJFNzF9L6jbr02sfVsDpLW0jJ1EKBLYRnjMkwOeHsaAB2Nvy5LegjU_heidihZ4ER6gMFvJwfemv5RPcUi6CTnrURO68aLYi-aWWi9UL_svHhs4wXP_Rc3Qe9-Fj0ezsGz0JiOTXlVnluTMQ3Z49TKv4uxEMNi573MqabS7hvBlaPC_3XvDg68d0ZwZjxCoZAUtMlFsozKAhQGG4_ptJjvbh5lliBVmE55h8NPsBWDu8tnS12_f7hFfQAsHEKPzEqv7AW1h17WIp-mls-gpUYFpSMvhXsoPRCexkqQP-Dw46RC2SUCUp_ueEhQ3gaDDHD6KzqVeQCMStfLK9rxpLlqA9p7SA4_udwEblTNsF50EH-rIcXqY3MIR-cZ0oDy9zJGNe9qb96WoNaf9AufoRl1oBEIhBb9QaJAd_rKAST8ykONFke4Aeuo5zNWygwCy5zEh8.obcZsXWymFz_WvFaZsX4pw/__results___files/__results___6_1.png" /></p>
<p>该数据集中的目标值是 MedHouseVal（房屋价值中位数）。这些方框图显示了目标值在每个聚类中的分布情况。如果聚类是有信息量的，那么这些分布在 MedHouseVal 上大部分应该是分开的，这正是我们所看到的。</p>
<pre><code class="language-python">
X[&quot;MedHouseVal&quot;] = df[&quot;MedHouseVal&quot;]
sns.catplot(x=&quot;MedHouseVal&quot;, y=&quot;Cluster&quot;, data=X, kind=&quot;boxen&quot;, height=6);

</code></pre>
<p><img alt="" src="https://www.kaggleusercontent.com/kf/137493070/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..vRcdbHQcAh2ZxHgNKZ5AYQ.JFIBuAVj7g0O9h0MbLjkDXjNmG2v98OLXuVmYnF6gBrzxyo3eVV_abvE2_MEjirnAYAS0CXQxQClrbRm--jwT6Cams6PboIV6JIgpxx9FYE4GiTB-dp3CFUccMmSAOQu6u-8FGVRNK1ogCUZuSobXTD9VA34lckxbgI9JFJBbCoJV4ASWPW9MNQfQs3eY7UnyILK-PXlpbyfUgcb4pkXt4Pw_iMW8ZSnrfs8B9Qp5EHvqyHgmPXHJv2iZeVjK3UckHp5CiQ2yLAv7MxzBzEpAdg_tJtFc8NeBbHT_-PykGEG5587dL9L9F6G8sdqfeGhNaifb2Be2niiYtKvUSCJQN-efIR2GswddY0B0fysHrl6fz_OFJhEYuCjgKWEQUFLoEB2qd9XRRrFDS85bI9uKuNEgWIRJ3I39w3aAjSEHRk_jTX1cdh4X24hMUu8NR7ORqjdCiyVwfIDvkdHgapnk3e2URRku61G9yMYBZiytsHEGY9EqnTlwM77KNpvCewzrDeazD4HEJrEbOD_vJYjl1s7vrR3mGMD3n_blG5DIuk0bMp30HdjGBqKHVFOOL4R9x3-MBN82QwcArmWF37UMaYmdVGYKz5NuD8XHvWE8ywxkBvq1JVHvgh9uuYY1roElG30F1_MFQ96esU0YfWrOwQK4Dj8P6lOgLmyvwIzrsQ.A83VQ0iR8wajsPDrhsC0pA/__results___files/__results___8_1.png" /></p>
<p>k-means 算法对数据的尺度（我不太清楚怎么形容它，可能被称为“数量级”更合适）很敏感。这就意味着我们需要深思熟虑，考虑如何以及是否要对特征进行重新缩放，因为根据我们的选择，可能会得到截然不同的结果。根据经验，如果特征已经具有直接可比性（比如不同时间的测试结果），那么就不需要重新缩放。另一方面，不具有可比性的特征（如身高和体重）通常会受益于重新缩放。有时，选择并不明确。在这种情况下，您应该尽量利用常识，记住值越大的特征权重越高。</p>
<h3 id="pac">主成分分析法（PAC）</h3>
<p>主成分分析法（PCA）就像聚类算法一样是基于接近性对数据集进行划分的一种方法，你也可以将 PCA 视为对数据中的相关性进行划分。PCA 是帮助您发现数据中重要关系的绝佳工具，也可用于创建信息量更大的特征。</p>
<p>（技术说明：PCA 通常应用于标准化数据。对于标准化数据，"变异 "意味着 "相关性"。对于非标准化数据，"变异 "指的是 "协方差"。本课程中的所有数据在应用 PCA 之前都将标准化）。</p>
<p>课程使用的 Abalone（鲍鱼）数据集中有几千个塔斯马尼亚鲍鱼的物理测量数据。（鲍鱼是一种海洋生物，很像蛤蜊或牡蛎。）我们现在只看几个特征：鲍鱼壳的 "高度 "和 "直径"。</p>
<p>您可以想象，在这些数据中，有一些 "变异轴 "描述了鲍鱼之间的差异。从图像上看，这些轴线是沿着数据的自然尺寸运行的垂直线，每个原始特征有一个轴线。</p>
<p><img alt="" src="https://storage.googleapis.com/kaggle-media/learn/images/rr8NCDy.png" /></p>
<p>通常，我们可以为这些变化轴命名。较长的轴线我们可以称为 "尺寸 "部分：小高度和小直径（左下）与大高度和大直径（右上）形成对比。较短的轴线我们可以称为 "形状 "部分：小高度和大直径（扁平形状）与大高度和小直径（圆形）形成对比。</p>
<p>请注意，与其用 "高度 "和 "直径 "来描述鲍鱼，不如用 "大小 "和 "形状 "来描述鲍鱼。事实上，这就是 PCA 的整个理念：我们不是用原始特征来描述数据，而是用数据的变异轴来描述它。变异轴就是新的特征。</p>
<p><img alt="" src="https://storage.googleapis.com/kaggle-media/learn/images/XQlRD1q.png" /></p>
<p>PCA 构建的新特征实际上只是原始特征的线性组合（加权和）：</p>
<pre><code class="language-python">df[&quot;Size&quot;] = 0.707 * X[&quot;Height&quot;] + 0.707 * X[&quot;Diameter&quot;]
df[&quot;Shape&quot;] = 0.707 * X[&quot;Height&quot;] - 0.707 * X[&quot;Diameter&quot;]
</code></pre>
<p>这些新特征被称为数据的主成分。这些权重被称之为载荷。原始数据集中有多少个特征，就会有多少个主成分：如果我们使用的是十个特征而不是两个，最终就会有十个成分。</p>
<p>一个成分的载荷告诉我们它通过符号和大小表达了什么变化：</p>
<table>
<thead>
<tr>
<th align="center">Features \ Components</th>
<th align="center">Size (PC1)</th>
<th align="center">Shape (PC2)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Height</td>
<td align="center">0.707</td>
<td align="center">0.707</td>
</tr>
<tr>
<td align="center">Diameter</td>
<td align="center">0.707</td>
<td align="center">-0.707</td>
</tr>
</tbody>
</table>
<p>这张载荷表告诉我们，在 "尺寸 "成分中，"高度 "和 "直径 "的变化方向相同（符号相同），但在 "形状 "成分中，它们的变化方向相反（符号相反）。在每个分量中，载荷的大小都相同，因此特征在两个分量中的作用相同。</p>
<p>PCA 还能告诉我们每个分量的变化量。从图中我们可以看出，数据中 "尺寸 "分量的变异大于 "形状 "分量。PCA 通过每个分量的解释方差百分比精确地说明了这一点。</p>
<p><img alt="" src="https://storage.googleapis.com/kaggle-media/learn/images/xWTvqDA.png" /></p>
<p>在高度和直径的差异中，大小约占 96%，形状约占 4%。</p>
<p>"Size"成分捕捉了 "身高 "和 "直径 "之间的大部分变异。不过，重要的是要记住，一个成分的变异量并不一定与它作为预测指标的好坏相对应：这取决于你试图预测什么。</p>
<h4 id="pca">PCA 在特征工程中的作用</h4>
<p>有两种方法可以将 PCA 用于特征工程。</p>
<p>第一种方法是将其用作描述性技术。由于成分可以说明变异情况，因此可以计算成分的 MI 分数，看看哪种变异对目标最有预测性。这可以为您提供创建特征类型的思路--例如，如果 "尺寸 "很重要，可以创建 "高度 "和 "直径 "的乘积；如果 "形状 "很重要，可以创建 "高度 "和 "直径 "的比率。您甚至可以尝试对一个或多个高分成分进行聚类。</p>
<p>第二种方法是使用成分本身作为特征。由于成分直接揭示了数据的变异结构，因此往往比原始特征更有信息量。下面是一些使用案例：</p>
<blockquote>
<ul>
<li>降维： 当特征高度冗余（特别是多重共线性特征）时，PCA 会将冗余划分为一个或多个接近零方差的成分，由于这些成分几乎不包含任何信息，因此可以将其丢弃。</li>
<li>异常检测： 在原始特征中并不明显的异常变化往往会在低方差成分中显现出来。在异常或离群点检测任务中，这些成分可能具有很高的信息量。</li>
<li>降低噪音： 传感器读数集合通常会有一些共同的背景噪声。PCA 有时可以将（信息量大的）信号收集到数量较少的特征中，而将噪声排除在外，从而提高信噪比。</li>
<li>去相关性： 有些多重特征法在处理高度相关的特征时会遇到困难。PCA 可将相关特征转化为不相关的成分，从而使算法更容易处理。</li>
</ul>
</blockquote>
<p>从根本上说，PCA 可以让您直接访问数据的相关结构。毫无疑问，您会想出自己的应用方法！</p>
<blockquote>
<p>PCA 最佳实践
应用 PCA 时需要注意以下几点：</p>
<blockquote>
<ul>
<li>PCA 只适用于数字特征，如连续量或计数。</li>
<li>PCA 对规模很敏感。在应用 PCA 之前，最好先将数据标准化，除非您有充分的理由不这样做。</li>
<li>考虑移除或限制异常值，因为它们会对结果产生不当影响</li>
</ul>
</blockquote>
</blockquote>
<h4 id="-1985">实例-1985年汽车</h4>
<p>在本例中，我们在汽车数据集，应用 PCA，将其作为描述性技术来发现新的特征。</p>
<pre><code class="language-python">#可视化及MI值计算定义
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from IPython.display import display
from sklearn.feature_selection import mutual_info_regression


plt.style.use(&quot;seaborn-whitegrid&quot;)
plt.rc(&quot;figure&quot;, autolayout=True)
plt.rc(
    &quot;axes&quot;,
    labelweight=&quot;bold&quot;,
    labelsize=&quot;large&quot;,
    titleweight=&quot;bold&quot;,
    titlesize=14,
    titlepad=10,
)


def plot_variance(pca, width=8, dpi=100):
    # Create figure
    fig, axs = plt.subplots(1, 2)
    n = pca.n_components_
    grid = np.arange(1, n + 1)
    # Explained variance
    evr = pca.explained_variance_ratio_
    axs[0].bar(grid, evr)
    axs[0].set(
        xlabel=&quot;Component&quot;, title=&quot;% Explained Variance&quot;, ylim=(0.0, 1.0)
    )
    # Cumulative Variance
    cv = np.cumsum(evr)
    axs[1].plot(np.r_[0, grid], np.r_[0, cv], &quot;o-&quot;)
    axs[1].set(
        xlabel=&quot;Component&quot;, title=&quot;% Cumulative Variance&quot;, ylim=(0.0, 1.0)
    )
    # Set up figure
    fig.set(figwidth=8, dpi=100)
    return axs

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name=&quot;MI Scores&quot;, index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores


df = pd.read_csv(&quot;../input/fe-course-data/autos.csv&quot;)
</code></pre>
<p>我们选择了四个特征，涵盖了一系列属性。这些特征中的每一个与目标价格的 MI 分数都很高。我们将对数据进行标准化处理，因为这些特征自然不在同一范围内。</p>
<pre><code class="language-python">

features = [&quot;highway_mpg&quot;, &quot;engine_size&quot;, &quot;horsepower&quot;, &quot;curb_weight&quot;]

X = df.copy()
y = X.pop('price')
X = X.loc[:, features]

# Standardize
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)


</code></pre>
<p>现在，我们可以拟合 scikit-learn 的 PCA 估计器并创建主成分。您可以看到转换后数据集的前几行。</p>
<pre><code class="language-python">from sklearn.decomposition import PCA

# Create principal components
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Convert to dataframe
component_names = [f&quot;PC{i+1}&quot; for i in range(X_pca.shape[1])]
X_pca = pd.DataFrame(X_pca, columns=component_names)

X_pca.head()
</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">PC1</th>
<th align="center">PC2</th>
<th align="center">PC3</th>
<th align="center">PC4</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">0</td>
<td align="center">0.382486</td>
<td align="center">-0.400222</td>
<td align="center">0.124122</td>
<td align="center">0.169539</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">0.382486</td>
<td align="center">-0.400222</td>
<td align="center">0.124122</td>
<td align="center">0.169539</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">1.550890</td>
<td align="center">-0.107175</td>
<td align="center">0.598361</td>
<td align="center">-0.256081</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">-0.408859</td>
<td align="center">-0.425947</td>
<td align="center">0.243335</td>
<td align="center">0.013920</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">1.132749</td>
<td align="center">-0.814565</td>
<td align="center">-0.202885</td>
<td align="center">0.224138</td>
</tr>
</tbody>
</table>
<p>拟合后，PCA 实例的 components_ 属性中就包含了载荷。（不幸的是，PCA 的术语并不一致。我们按照惯例将 X_pca 中的转换列称为成分，否则它们就没有名称）。我们将用一个Dataframe来封装载荷。</p>
<pre><code class="language-python">
loadings = pd.DataFrame(
    pca.components_.T,  # transpose the matrix of loadings
    columns=component_names,  # so the columns are the principal components
    index=X.columns,  # and the rows are the original features
)
loadings

</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">PC1</th>
<th align="center">PC2</th>
<th align="center">PC3</th>
<th align="center">PC4</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">highway_mpg</td>
<td align="center">-0.492347</td>
<td align="center">0.770892</td>
<td align="center">0.070142</td>
<td align="center">-0.397996</td>
</tr>
<tr>
<td align="center">engine_size</td>
<td align="center">0.503859</td>
<td align="center">0.626709</td>
<td align="center">0.019960</td>
<td align="center">0.594107</td>
</tr>
<tr>
<td align="center">horsepower</td>
<td align="center">0.500448</td>
<td align="center">0.013788</td>
<td align="center">0.731093</td>
<td align="center">-0.463534</td>
</tr>
<tr>
<td align="center">curb_weight</td>
<td align="center">0.503262</td>
<td align="center">0.113008</td>
<td align="center">-0.678369</td>
<td align="center">-0.523232</td>
</tr>
</tbody>
</table>
<p>回想一下，一个成分载荷的符号和大小可以告诉我们它捕捉到了哪种变化。第一个分量（PC1）显示了油耗较低的大型、动力强劲的汽车与油耗较高的小型、更经济的汽车之间的对比。我们可以称之为 "豪华/经济 "轴。下图显示，我们选择的四个特征大多沿 "豪华/经济 "轴变化。</p>
<p><img alt="" src="https://www.kaggleusercontent.com/kf/126574304/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..nMYR78bA7GRmTrIsyBYe-A.zb_ABVm3kUK69CH2ymwIyaP49fDTnj5nEk_ZkLdB7xcTbf9Nqwp3pav1IVYUJ2mHWxsi5pib1YGnRZTSQw-kzqPWRC9SOWdGquPaMvuvOPBm3Dn1CJPQkz_k7Wm0JTgsylY4NIAuVen2ktyH_4hIbGtytrv2-q9gOK52wPY1_IwnmqGf25k1s5HSaf-82SEvmcIFbanO_WDuX10u7IThxn2K-7nDyF0FQOycZc6k20bgUw7tHSV-HnZX4XmJv_u9IA1tRMt0SsYVU07w4PSgOg1kRlfbyXn2lNqZ9j-vVDqLLn34qUzCtGNxotqV-oQUtE4UYWaefkUJK_OzxkZ55tK5Fd9jBecwBZFOee40Woh62c3unzdwVCnH-EIUdJvTsthkT2TF97cDMmtdVJ0n0nd7NCL5wbeAYvtt2UZDOC34Er1eovF2eGqJTSLeRq5mtoYxScGyxlH1fuj5cNxAQ36PnJ7tqVl4jmy-n1iVTDc02iaXPKfPtGtlCvI5A-6K5fmnP_8PFnQ2twDMg_6O2-hRFv28acL5MNReSvVI2p-rZkmT9KnYA8o2_Adhy_uB--V8fu2hFMtGAPF_Am-D-4TPpTG3t4rIrg_4DGQKLgbWPA2lH2X9755U7KzJW-LwDV1-I-E-WM2jPal21uKTglUDa5WQoQ3_zN-wvdEcUf8O8_MTiyIgdZc4oq844o4W.5JYuCZgTyFrsq00X1QclaQ/__results___files/__results___11_0.png" /></p>
<p>我们再来看看各成分的 MI 分数。不难看出，PC1 的信息量非常大，尽管其余成分的方差较小，但它们与价格的关系仍然显著。对这些成分进行研究，可以发现 "豪华/经济 "主轴没有反映的关系。</p>
<pre><code class="language-python">mi_scores = make_mi_scores(X_pca, y, discrete_features=False)
mi_scores
</code></pre>
<blockquote>
<p>PC1    1.013264
PC2    0.379156
PC3    0.306703
PC4    0.203329
Name: MI Scores, dtype: float64</p>
</blockquote>
<p>第三个部分显示了马力和整备质量之间的对比 -- 似乎是跑车和旅行车的对比。</p>
<pre><code class="language-python">

# Show dataframe sorted by PC3
idx = X_pca[&quot;PC3&quot;].sort_values(ascending=False).index
cols = [&quot;make&quot;, &quot;body_style&quot;, &quot;horsepower&quot;, &quot;curb_weight&quot;]
df.loc[idx, cols]


</code></pre>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">make</th>
<th align="center">body_style</th>
<th align="center">horsepower</th>
<th align="center">curb_weight</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">118</td>
<td align="center">porsche</td>
<td align="center">hardtop</td>
<td align="center">207</td>
<td align="center">2756</td>
</tr>
<tr>
<td align="center">117</td>
<td align="center">porsche</td>
<td align="center">hardtop</td>
<td align="center">207</td>
<td align="center">2756</td>
</tr>
<tr>
<td align="center">119</td>
<td align="center">porsche</td>
<td align="center">convertible</td>
<td align="center">207</td>
<td align="center">2800</td>
</tr>
<tr>
<td align="center">45</td>
<td align="center">jaguar</td>
<td align="center">sedan</td>
<td align="center">262</td>
<td align="center">3950</td>
</tr>
<tr>
<td align="center">96</td>
<td align="center">nissan</td>
<td align="center">hatchback</td>
<td align="center">200</td>
<td align="center">3139</td>
</tr>
<tr>
<td align="center">...</td>
<td align="center">...</td>
<td align="center">...</td>
<td align="center">...</td>
<td align="center">...</td>
</tr>
<tr>
<td align="center">59</td>
<td align="center">mercedes-benz</td>
<td align="center">wagon</td>
<td align="center">123</td>
<td align="center">3750</td>
</tr>
<tr>
<td align="center">61</td>
<td align="center">mercedes-benz</td>
<td align="center">sedan</td>
<td align="center">123</td>
<td align="center">3770</td>
</tr>
<tr>
<td align="center">101</td>
<td align="center">peugot</td>
<td align="center">wagon</td>
<td align="center">95</td>
<td align="center">3430</td>
</tr>
<tr>
<td align="center">105</td>
<td align="center">peugot</td>
<td align="center">wagon</td>
<td align="center">95</td>
<td align="center">3485</td>
</tr>
<tr>
<td align="center">143</td>
<td align="center">toyota</td>
<td align="center">wagon</td>
<td align="center">62</td>
<td align="center">3110</td>
</tr>
</tbody>
</table>
<p>为了表达这种对比，让我们创建一个新的比率特征：</p>
<pre><code class="language-python">

df[&quot;sports_or_wagon&quot;] = X.curb_weight / X.horsepower
sns.regplot(x=&quot;sports_or_wagon&quot;, y='price', data=df, order=2);


</code></pre>
<p><img alt="" src="https://www.kaggleusercontent.com/kf/126574304/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..nMYR78bA7GRmTrIsyBYe-A.zb_ABVm3kUK69CH2ymwIyaP49fDTnj5nEk_ZkLdB7xcTbf9Nqwp3pav1IVYUJ2mHWxsi5pib1YGnRZTSQw-kzqPWRC9SOWdGquPaMvuvOPBm3Dn1CJPQkz_k7Wm0JTgsylY4NIAuVen2ktyH_4hIbGtytrv2-q9gOK52wPY1_IwnmqGf25k1s5HSaf-82SEvmcIFbanO_WDuX10u7IThxn2K-7nDyF0FQOycZc6k20bgUw7tHSV-HnZX4XmJv_u9IA1tRMt0SsYVU07w4PSgOg1kRlfbyXn2lNqZ9j-vVDqLLn34qUzCtGNxotqV-oQUtE4UYWaefkUJK_OzxkZ55tK5Fd9jBecwBZFOee40Woh62c3unzdwVCnH-EIUdJvTsthkT2TF97cDMmtdVJ0n0nd7NCL5wbeAYvtt2UZDOC34Er1eovF2eGqJTSLeRq5mtoYxScGyxlH1fuj5cNxAQ36PnJ7tqVl4jmy-n1iVTDc02iaXPKfPtGtlCvI5A-6K5fmnP_8PFnQ2twDMg_6O2-hRFv28acL5MNReSvVI2p-rZkmT9KnYA8o2_Adhy_uB--V8fu2hFMtGAPF_Am-D-4TPpTG3t4rIrg_4DGQKLgbWPA2lH2X9755U7KzJW-LwDV1-I-E-WM2jPal21uKTglUDa5WQoQ3_zN-wvdEcUf8O8_MTiyIgdZc4oq844o4W.5JYuCZgTyFrsq00X1QclaQ/__results___files/__results___17_0.png" /></p>
<p><img alt="" src="https://www.kaggleusercontent.com/kf/126574304/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..kEdZAhOV90XckQiYr2RQqw.Ujb04KVjpZog9MX1apPBHSEHV5TlrTd0lrgv9OMzuf6yUoOczxptAJYRoPNDzs_YNik_iFp3xHHBZXnTD_TzVmvp62-RH3cLE-z_Lb_pom1AP-ZzLwrDCnC6PRY6dsQ80Dn8IXZyx6vfer4rbXT0kfi-pWNZUx51CmGfHkXa_CC3U4eybxMdEPK7NcPvmjXMSb-75L6IYtbFYhGufwxOYIiw_OyabxSoftGWbM3O_ZdglNJUf94L5TmqCtPFoh6_WoTOtVdabpNp6ugtrvH5Sdjwg12oo5mJ94LSKSiKENoEm_nW3m7i9M1gG_THCKEENLhiyMmvfBOi8BmYl750JpWzHOEalrLxf3vyqqLptpeJnbl1bo3YMEP7uS5vFpsevM3Lwfxu-gEZossT8xGVgn2TMkFwxIE2I2AY0JKCH-kIvvqH8YW0t3ICUIF4RPCDyvzoMDRQdJ2QcpPojYAQx5lbDq66Mk8rZe6U92Dgtag2r4e_wBhAKw-WXgulr0V6mpAJ5BKriHHvJ2PEmw0KyorJfrys8WoGKfRhHsNkdeS-roTJeHYh_1qJY-MBGcJ157WZ8g2-P4Kiw2qljmkD_JMLMlL5JPRzdkjDQpevEfkw9NNBXix4R69InR3QvNuHG8jVyYRXQK7vc6-aebs-2OX53RmhRIeObtripQLLFOeQNR0IU2BqfUWsC5tn92nG.liEJAKK-cRNqfiIw7GLpxw/__results___files/__results___17_0.png" /></p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
